{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "print (tf.__version__)\n",
    "import Dataset\n",
    "import time\n",
    "import os\n",
    "import configparser as ConfigParser\n",
    "import sys\n",
    "import argparse \n",
    "import psutil\n",
    "import  resnet18_linknet as ln"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hallucinator ():    \n",
    "    def __init__ (self,config_file,scale,gpu_num):\n",
    "        print (\"Initializing Hallucinator class\")\n",
    "        self.scale=scale\n",
    "        self.gpu =\"/gpu:{}\".format(gpu_num)\n",
    "        self.readConfiguration(config_file)\n",
    "        self.depth=tf.placeholder(tf.float32,(None,self.imageHeight,self.imageWidth,self.channels),name='depthInput')             \n",
    "        self.phase=tf.placeholder(tf.bool)\n",
    "        self.rgb  =tf.placeholder(tf.float32,(None,self.imageHeight,self.imageWidth,self.channels),name='grountTruth')\n",
    "        self.dataObj = Dataset.dataReader(self.dataArguments)\n",
    "        if not os.path.exists(self.summary_writer_dir):\n",
    "            os.makedirs(self.summary_writer_dir)\n",
    "        if not os.path.exists(self.modelLocation):\n",
    "            os.makedirs(self.modelLocation)\n",
    "        logPath = os.path.join(self.logDir,self.modelName)\n",
    "        if not os.path.exists(logPath):\n",
    "            os.makedirs(logPath)\n",
    "        self.logDir = os.path.join(logPath,'log.txt')\n",
    "        \n",
    "        os.environ['CUDA_VISIBLE_DEVICES'] = str(gpu_num)\n",
    "        self.sess = None\n",
    "        with open(config_file) as fh :\n",
    "            self.confInfo = fh.read()\n",
    "        if self.model_choice == \"linkNet\":\n",
    "            linkNet = ln.LinkNet_resnt18(self.depth, is_training=self.phase,num_classes =self.channels)\n",
    "            out, end_points = linkNet.build_model()\n",
    "            self.model = out\n",
    "        if self.model_choice == \"APG\" :\n",
    "            self.model = self.generateImage()\n",
    "            \n",
    "    def readConfiguration(self,config_file):\n",
    "        print (\"Reading configuration File\")\n",
    "        config = ConfigParser.ConfigParser()\n",
    "        config.read(config_file)\n",
    "        self.imageWidth=int(int(config.get('DATA','imageWidth'))/self.scale)\n",
    "        self.imageHeight=int(int(config.get('DATA','imageHeight'))/self.scale)\n",
    "        self.channels=int (config.get('DATA','channels'))\n",
    "        self.train_file = config.get ('DATA','train_file')\n",
    "        self.test_file  = config.get ('DATA','test_file')\n",
    "        self.batchSize  = int(config.get ('DATA','batchSize'))\n",
    "        self.dataArguments = {\"imageWidth\":self.imageWidth,\"imageHeight\":self.imageHeight,\"channels\" : self.channels, \"batchSize\":self.batchSize,\"train_file\":self.train_file,\"test_file\":self.test_file,\"scale\":self.scale}    \n",
    "        self.maxEpoch=int(config.get('TRAIN','maxEpoch'))\n",
    "        self.learningRate = float(config.get('TRAIN','learningRate'))\n",
    "        self.huberDelta  = float(config.get('TRAIN','huberDelta'))\n",
    "        self.lambda1     = float(config.get('TRAIN','rmse_lambda'))\n",
    "        self.lambda2     = float(config.get('TRAIN','smooth_lambda'))\n",
    "        if int(config.get('TRAIN','activation')) == 0:\n",
    "            self.activation = tf.nn.relu\n",
    "            print(\"Relu activation has been chosen\")\n",
    "        else :\n",
    "            self.activation = tf.nn.selu\n",
    "        self.normType = config.get('TRAIN',\"normalizationType\")\n",
    "        print (self.normType + \" Normalization has been chosen\")\n",
    "        self.print_freq=int(config.get('LOG','print_freq'))\n",
    "        self.save_freq = int (config.get('LOG','save_freq'))\n",
    "        self.val_freq = int (config.get('LOG','val_freq'))\n",
    "        self.modelName = config.get('LOG','modelName') +\"_s{}\".format(self.scale)\n",
    "        self.modelLocation = config.get('LOG','modelLocation')\n",
    "        self.modelLocation = os.path.join(self.modelLocation , self.modelName)\n",
    "        self.checkPoint =  bool(int(config.get('LOG','checkpoint')))\n",
    "        #self.restoreModelPath =config.get('LOG','restoreModelPath')\n",
    "        self.logDir = config.get('LOG','logFile')\n",
    "        \n",
    "        if self.checkPoint:\n",
    "            print (\"Using the latest trained model in check point file\")\n",
    "            self.restoreModelPath = tf.train.latest_checkpoint(self.modelLocation)\n",
    "            print (\" Model at {} restored\".format(self.restoreModelPath))\n",
    "        else : \n",
    "            self.restoreModelPath = config.get('LOG','restoreModelPath')\n",
    "        self.summary_writer_dir =os.path.join(config.get('LOG','summary_writer_dir') ,self.modelName)    \n",
    "        self.model_choice = config.get('TRAIN','model')\n",
    "        \n",
    "        \n",
    "        \n",
    "    def normalization(self,feat,typeN):\n",
    "        if typeN == \"BATCH\":\n",
    "            return tf.layers.batch_normalization(feat, training = self.phase)\n",
    "        if typeN == \"INSTANCE\":\n",
    "            return tf.contrib.layers.instance_norm(feat)\n",
    "            \n",
    "#     def generateImage(self):  # Inference procedure\n",
    "#         with tf.variable_scope(self.modelName, reuse=tf.AUTO_REUSE):\n",
    "#             #layer 1\n",
    "#             conv1 = tf.layers.conv2d(inputs=self.depth,filters=147,kernel_size=(11,11), padding=\"same\",name=\"conv1\",kernel_initializer=tf.truncated_normal_initializer)\n",
    "#             conv1 = self.normalization(conv1,self.normType) \n",
    "#             conv1 = self.activation(conv1,name=\"conv1_actvn\")\n",
    "            \n",
    "#             #layer 2\n",
    "#             conv2 = tf.layers.conv2d(inputs=conv1,filters=36,kernel_size=(11,11),padding=\"same\",name=\"conv2\",kernel_initializer=tf.truncated_normal_initializer)\n",
    "#             conv2 = self.normalization(conv2,self.normType)\n",
    "#             conv2 = self.activation(conv2,name='conv2_actvn')\n",
    "            \n",
    "#             #layer 3\n",
    "#             conv3 = tf.layers.conv2d(inputs=conv2,filters=36,kernel_size=(11,11),padding=\"same\",name=\"conv3\",kernel_initializer=tf.truncated_normal_initializer)\n",
    "#             conv3 = self.normalization(conv3,self.normType)\n",
    "#             conv3 = self.activation(conv3,name='conv3_actvn')\n",
    "            \n",
    "#             #layer 4\n",
    "#             conv4 = tf.layers.conv2d(inputs=conv3,filters=36,kernel_size=(11,11),padding=\"same\",name=\"conv4\",kernel_initializer=tf.truncated_normal_initializer)\n",
    "#             conv4 = self.normalization(conv4,self.normType)\n",
    "#             conv4 = self.activation(conv4,name=\"conv4_actvn\")\n",
    "            \n",
    "#             #layer 5\n",
    "#             conv5 = tf.layers.conv2d(inputs=conv4,filters=36,kernel_size=(11,11),padding=\"same\",name=\"conv5\",kernel_initializer=tf.truncated_normal_initializer)\n",
    "#             conv5 = self.normalization(conv5,self.normType)\n",
    "#             conv5 = self.activation(conv5,name=\"conv5_actvn\")\n",
    "            \n",
    "#             #layer 6 \n",
    "#             conv6 = tf.layers.conv2d(inputs=conv5,filters=36,kernel_size=(11,11),padding=\"same\",name=\"conv6\",kernel_initializer=tf.truncated_normal_initializer)\n",
    "#             conv6 = self.normalization(conv6,self.normType)\n",
    "#             conv6 = self.activation(conv6,name=\"conv6_actvn\")\n",
    "            \n",
    "#             #layer 7\n",
    "#             conv7 = tf.layers.conv2d(inputs=conv6,filters=36,kernel_size=(11,11),padding=\"same\",name=\"conv7\",kernel_initializer=tf.truncated_normal_initializer)\n",
    "#             conv7 = self.normalization(conv7,self.normType)\n",
    "#             conv7 = self.activation(conv7,name=\"conv7_actvn\")\n",
    "            \n",
    "#             #layer 8\n",
    "#             conv8 = tf.layers.conv2d(inputs=conv7,filters=36,kernel_size=(11,11),padding=\"same\",name=\"conv8\",kernel_initializer=tf.truncated_normal_initializer)\n",
    "#             conv8 = self.normalization(conv8,self.normType)\n",
    "#             conv8 = self.activation(conv8,name=\"conv8_actvn\")\n",
    "            \n",
    "#             #layer 9 \n",
    "#             conv9 = tf.layers.conv2d(inputs=conv8,filters=36,kernel_size=(11,11),padding=\"same\",name=\"conv9\",kernel_initializer=tf.truncated_normal_initializer)\n",
    "#             conv9 = self.normalization(conv9,self.normType)\n",
    "#             conv9 = self.activation(conv9,name=\"conv9_actvn\")\n",
    "            \n",
    "#             #layer 10\n",
    "#             conv10 = tf.layers.conv2d(inputs=conv9,filters=147,kernel_size=(11,11),padding=\"same\",name=\"conv10\",kernel_initializer=tf.truncated_normal_initializer)\n",
    "#             conv10 = self.normalization(conv10,self.normType)\n",
    "#             conv10 = self.activation(conv10,name=\"conv10_actvn\")\n",
    "            \n",
    "#             #Image generation layer \n",
    "#             outH = tf.layers.conv2d(inputs=conv10,filters=3,kernel_size=(11,11),padding=\"same\",name=\"output\",kernel_initializer=tf.truncated_normal_initializer)\n",
    "#             return outH\n",
    "        \n",
    "    def generateImage(self):\n",
    "        with tf.variable_scope(self.modelName, reuse=tf.AUTO_REUSE):\n",
    "            mu,sigma=0,0.1\n",
    "            strides=[1,1,1,1]\n",
    "            #Layer 1 \n",
    "            w1=tf.Variable(tf.truncated_normal(shape=(11,11,3,147),mean=mu,stddev= sigma))\n",
    "            b1=tf.Variable(tf.zeros(147))\n",
    "            conv1=tf.nn.conv2d(self.depth,w1,strides,padding='SAME')\n",
    "            conv1=tf.nn.bias_add(conv1,b1)\n",
    "            conv1=self.normalization(conv1,self.normType)\n",
    "            conv1=self.activation(conv1)\n",
    "            #chk1=tf.check_numerics(conv1,\"conv1\")\n",
    "            #Layer 2\n",
    "            w2=tf.Variable(tf.truncated_normal(shape=(11,11,147,36),mean=mu,stddev=sigma))\n",
    "            b2=tf.Variable(tf.zeros(36))\n",
    "            conv2=tf.nn.conv2d(conv1,w2,strides,padding='SAME')\n",
    "            conv2=tf.nn.bias_add(conv2,b2)\n",
    "            conv2=self.normalization(conv2,self.normType)\n",
    "            conv2=self.activation(conv2)\n",
    "            #Layer 3\n",
    "            w3=tf.Variable(tf.truncated_normal(shape=(11,11,36,36),mean=mu,stddev=sigma))\n",
    "            b3=tf.Variable(tf.zeros(36))\n",
    "            conv3=tf.nn.conv2d(conv2,w3,strides,padding='SAME')\n",
    "            conv3=tf.nn.bias_add(conv3,b3)\n",
    "            conv3=self.normalization(conv3,self.normType)\n",
    "            conv3=self.activation(conv3)\n",
    "            #Layer 4\n",
    "            w4=tf.Variable(tf.truncated_normal(shape=(11,11,36,36),mean=mu,stddev=sigma))\n",
    "            b4=tf.Variable(tf.zeros(36))\n",
    "            conv4=tf.nn.conv2d(conv3,w4,strides,padding='SAME')\n",
    "            conv4=tf.nn.bias_add(conv4,b4)\n",
    "            conv4=self.normalization(conv4,self.normType)\n",
    "            conv4=self.activation(conv4)\n",
    "            #Layer 5\n",
    "            w5=tf.Variable(tf.truncated_normal(shape=(11,11,36,36),mean=mu,stddev=sigma))\n",
    "            b5=tf.Variable(tf.zeros(36))\n",
    "            conv5=tf.nn.conv2d(conv4,w5,strides,padding='SAME')\n",
    "            conv5=tf.nn.bias_add(conv5,b5)\n",
    "            conv5=self.normalization(conv5,self.normType)\n",
    "            conv5=self.activation(conv5)\n",
    "            #Layer 6\n",
    "            w6=tf.Variable(tf.truncated_normal(shape=(11,11,36,36),mean=mu,stddev=sigma))\n",
    "            b6=tf.Variable(tf.zeros(36))\n",
    "            conv6=tf.nn.conv2d(conv5,w6,strides,padding='SAME')\n",
    "            conv6=tf.nn.bias_add(conv6,b6)\n",
    "            conv6=self.normalization(conv6,self.normType)\n",
    "            conv6=self.activation(conv6)\n",
    "            #Layer 7\n",
    "            w7=tf.Variable(tf.truncated_normal(shape=(11,11,36,36),mean=mu,stddev=sigma))\n",
    "            b7=tf.Variable(tf.zeros(36))\n",
    "            conv7 =tf.nn.conv2d(conv6,w7,strides,padding='SAME')\n",
    "            conv7=tf.nn.bias_add(conv7,b7)\n",
    "            conv7=self.normalization(conv7,self.normType)\n",
    "            conv7=self.activation(conv7)\n",
    "            #layer 8\n",
    "            w8=tf.Variable(tf.truncated_normal(shape=(11,11,36,36),mean=mu,stddev=sigma))\n",
    "            b8=tf.Variable(tf.zeros(36))\n",
    "            conv8 = tf.nn.conv2d(conv7,w8,strides,padding='SAME')\n",
    "            conv8 = tf.nn.bias_add(conv8,b8)\n",
    "            conv8 = self.normalization(conv8,self.normType)\n",
    "            conv8 = self.activation(conv8)\n",
    "            #layer 9\n",
    "            w9=tf.Variable(tf.truncated_normal(shape=(11,11,36,36),mean=mu,stddev=sigma))\n",
    "            b9=tf.Variable(tf.zeros(36))\n",
    "            conv9 = tf.nn.conv2d(conv8,w9,strides,padding='SAME')\n",
    "            conv9 = tf.nn.bias_add(conv9,b9)\n",
    "            conv9 = self.normalization(conv9,self.normType)\n",
    "            conv9 = self.activation(conv9)\n",
    "            #layer 10\n",
    "            w10=tf.Variable(tf.truncated_normal(shape=(11,11,36,147),mean=mu,stddev=sigma))\n",
    "            b10=tf.Variable(tf.zeros(147))\n",
    "            conv10 =tf.nn.conv2d(conv9,w10,strides,padding='SAME')\n",
    "            conv10 = tf.nn.bias_add(conv10,b10)\n",
    "            conv10 = self.normalization(conv10,self.normType)\n",
    "            conv10 = self.activation(conv10)\n",
    "            #layer 11\n",
    "            w11=tf.Variable(tf.truncated_normal(shape=(11,11,147,3),mean=mu,stddev=sigma))\n",
    "            b11=tf.Variable(tf.zeros(3))\n",
    "            conv11 = tf.nn.conv2d(conv10,w11,strides,padding='SAME')\n",
    "            conv11 = tf.nn.bias_add(conv11,b11,name=\"output\")\n",
    "            return conv11\n",
    "    \n",
    "    def train(self):     \n",
    "        self.logger = open(self.logDir,'w')\n",
    "        self.logger.write(self.confInfo +'\\n\\n\\n')\n",
    "        self.outH=self.model\n",
    "        loss= self.loss()\n",
    "        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "        optimizer=tf.train.AdamOptimizer(learning_rate=self.learningRate)\n",
    "        with tf.control_dependencies(update_ops): \n",
    "            Trainables=optimizer.minimize(loss)\n",
    "        valid_image_summary =tf.summary.image('test_image_output',self.outH)\n",
    "        loss_summary = tf.summary.scalar('Loss',loss)\n",
    "        iters=0\n",
    "        self.saver = tf.train.Saver()\n",
    "        config=tf.ConfigProto()\n",
    "        config.gpu_options.allow_growth=True\n",
    "        self.sess = tf.Session(config=config)\n",
    "        train_summary_writer=tf.summary.FileWriter(os.path.join(self.summary_writer_dir,'train'),self.sess.graph)\n",
    "        test_summary_writer=tf.summary.FileWriter(os.path.join(self.summary_writer_dir,'test'),self.sess.graph)\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        process = psutil.Process(os.getpid())\n",
    "        while not self.dataObj.epoch == self.maxEpoch :\n",
    "            iters+=1\n",
    "            inp,gt = self.dataObj.nextTrainBatch()\n",
    "            t1=time.time()\n",
    "            _,lval,t_summaries = self.sess.run([Trainables,loss,loss_summary], feed_dict= {self.depth : inp,self.phase : True ,self.rgb : gt})\n",
    "            train_summary_writer.add_summary(t_summaries,iters)\n",
    "            t2=time.time()      \n",
    "            if not iters % self.print_freq:\n",
    "                info = \"Model Hallucinator_s{} Epoch  {} : Iteration : {}/{} loss value : {:0.4f} \\n\".format(self.scale,self.dataObj.epoch,iters,(self.maxEpoch)*int(self.dataObj.dataLength/self.dataObj.batchSize),lval) +\"Memory used : {:0.4f} GB Time per batch : {:0.3f}s\\n\".format(process.memory_info().rss/100000000.,t2-t1) \n",
    "                print (info)   \n",
    "                self.logger.write(info)\n",
    "                \n",
    "            if not iters % self.save_freq:\n",
    "                info=\"Model Saved at iteration {}\\n\".format(iters)\n",
    "                print (info)\n",
    "                self.logger.write(info)\n",
    "                self.saveModel(iters)\n",
    "                \n",
    "            if not iters % self.val_freq :\n",
    "                val_inp,val_gt  = self.dataObj.nextTestBatch()\n",
    "                val_loss,v_summaries,v_img_summaries = self.sess.run([loss,loss_summary,valid_image_summary],feed_dict={self.depth : val_inp,self.rgb : val_gt,self.phase:False})\n",
    "                test_summary_writer.add_summary(v_summaries, iters)\n",
    "                test_summary_writer.add_summary(v_img_summaries,iters)\n",
    "                info = \"Validation Loss at iteration{} : {}\\n\".format(iters, val_loss)\n",
    "                print (info)\n",
    "                self.logger.write(info)\n",
    "        print (\"Training done \")\n",
    "        self.saveModel(iters)\n",
    "        self.logger.close()\n",
    "    \n",
    "    def testAll(self):\n",
    "        self.restoreModel()\n",
    "        loss=[]\n",
    "        while not self.dataObj.test_epoch == 1 :\n",
    "            x,y = self.dataObj.nextTestBatch()\n",
    "            l = self.sess.run(self.loss(),feed_dict= {self.depth : x, self.rgb :y,self.phase : False})\n",
    "            loss.append(l)\n",
    "            print(\"Test Loss : {}\".format(l))\n",
    "        return np.mean(loss)\n",
    "        \n",
    "    def getHallucinatedImages(self,image_list):\n",
    "        #with tf.device(self.gpu):\n",
    "        self.restoreModel()\n",
    "        img_processed= self.dataObj.loadImages(image_list)\n",
    "        #generator = self.generateImage()\n",
    "        output = self.sess.run(self.outH,{self.depth:img_processed,self.phase : False})\n",
    "        return output\n",
    "    def loss_ (self) :\n",
    "        return tf.reduce_mean(2*tf.nn.l2_loss(self.outH-self.rgb))#/(img_ht*img_w*BATCH_SIZE*3)\n",
    "    def loss (self) :\n",
    "        return self.lambda1*self.l2_loss() + self.lambda2*self.smoothing_loss()\n",
    "\n",
    "    def l2_loss(self):\n",
    "        return tf.sqrt(tf.losses.mean_squared_error(self.rgb,self.outH))   \n",
    "\n",
    "    def smoothing_loss(self):\n",
    "        I_Hgrad    = tf.image.sobel_edges(self.outH)\n",
    "        I_Hedge    = I_Hgrad[:,:,:,:,0] + I_Hgrad[:,:,:,:,1]\n",
    "        zeros      = tf.zeros_like(I_Hedge)\n",
    "        I_Hhuber   = tf.losses.huber_loss(I_Hedge,zeros,delta = self.huberDelta,reduction=tf.losses.Reduction.NONE)\n",
    "            \n",
    "        I_RGBgrad  = tf.image.sobel_edges(self.rgb)\n",
    "        I_RGBedge  = I_RGBgrad[:,:,:,:,0] + I_RGBgrad[:,:,:,:,1]\n",
    "        I_RGBhuber = tf.losses.huber_loss(I_RGBedge,zeros,delta = self.huberDelta, reduction=tf.losses.Reduction.NONE)\n",
    "\n",
    "        edge_aware_weight   = tf.exp(-1*I_RGBhuber)\n",
    "        weighted_smooth_img = tf.multiply(I_Hhuber, edge_aware_weight)\n",
    "        loss_val = tf.reduce_mean(weighted_smooth_img) \n",
    "        return loss_val\n",
    "    def saveModel(self,iters):\n",
    "        if not os.path.exists (self.modelLocation):\n",
    "            os.makedirs(self.modelLocation)\n",
    "        self.saver.save(self.sess,os.path.join(self.modelLocation,self.modelName),global_step = iters)\n",
    "        \n",
    "    def restoreModel(self):\n",
    "        print (self.modelLocation)\n",
    "        if not self.sess is None:\n",
    "            if self.sess._opened :\n",
    "                self.sess.close()\n",
    "\n",
    "        #tf.reset_default_graph()\n",
    "        sess=tf.Session()\n",
    "        self.outH = self.model\n",
    "        saver=tf.train.Saver()\n",
    "        saver.restore(sess,self.restoreModelPath)\n",
    "        self.sess=sess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Hallucinator class\n",
      "Reading configuration File\n",
      "Relu activation has been chosen\n",
      "BATCH Normalization has been chosen\n",
      "Using the latest trained model in check point file\n",
      " Model at None restored\n",
      "Initializing Data Reader\n",
      "Train files 108256\n",
      "Test  files 27064\n",
      "Initialization Complete\n",
      "Model Hallucinator_s1 Epoch  0 : Iteration : 5/54120 loss value : 122.9214 \n",
      "Memory used : 30.2687 GB Time per batch : 0.333s\n",
      "\n",
      "Model Hallucinator_s1 Epoch  0 : Iteration : 10/54120 loss value : 116.0604 \n",
      "Memory used : 30.2697 GB Time per batch : 0.342s\n",
      "\n",
      "Model Hallucinator_s1 Epoch  0 : Iteration : 15/54120 loss value : 127.1853 \n",
      "Memory used : 30.2698 GB Time per batch : 0.327s\n",
      "\n",
      "Model Hallucinator_s1 Epoch  0 : Iteration : 20/54120 loss value : 130.4958 \n",
      "Memory used : 30.2698 GB Time per batch : 0.350s\n",
      "\n",
      "Model Hallucinator_s1 Epoch  0 : Iteration : 25/54120 loss value : 131.3004 \n",
      "Memory used : 30.2699 GB Time per batch : 0.337s\n",
      "\n",
      "Model Hallucinator_s1 Epoch  0 : Iteration : 30/54120 loss value : 127.1921 \n",
      "Memory used : 30.2699 GB Time per batch : 0.326s\n",
      "\n",
      "Model Hallucinator_s1 Epoch  0 : Iteration : 35/54120 loss value : 122.5981 \n",
      "Memory used : 30.2699 GB Time per batch : 0.335s\n",
      "\n",
      "Model Hallucinator_s1 Epoch  0 : Iteration : 40/54120 loss value : 128.4329 \n",
      "Memory used : 30.2703 GB Time per batch : 0.338s\n",
      "\n",
      "Model Hallucinator_s1 Epoch  0 : Iteration : 45/54120 loss value : 136.2998 \n",
      "Memory used : 30.2703 GB Time per batch : 0.330s\n",
      "\n",
      "Model Hallucinator_s1 Epoch  0 : Iteration : 50/54120 loss value : 128.2099 \n",
      "Memory used : 30.2703 GB Time per batch : 0.344s\n",
      "\n",
      "Model Hallucinator_s1 Epoch  0 : Iteration : 55/54120 loss value : 120.8122 \n",
      "Memory used : 30.2705 GB Time per batch : 0.336s\n",
      "\n",
      "Model Hallucinator_s1 Epoch  0 : Iteration : 60/54120 loss value : 125.4413 \n",
      "Memory used : 30.2705 GB Time per batch : 0.339s\n",
      "\n",
      "Model Hallucinator_s1 Epoch  0 : Iteration : 65/54120 loss value : 124.2217 \n",
      "Memory used : 30.2708 GB Time per batch : 0.335s\n",
      "\n",
      "Model Hallucinator_s1 Epoch  0 : Iteration : 70/54120 loss value : 135.9212 \n",
      "Memory used : 30.2709 GB Time per batch : 0.368s\n",
      "\n",
      "Model Hallucinator_s1 Epoch  0 : Iteration : 75/54120 loss value : 134.3837 \n",
      "Memory used : 30.2709 GB Time per batch : 0.348s\n",
      "\n",
      "Model Hallucinator_s1 Epoch  0 : Iteration : 80/54120 loss value : 130.5960 \n",
      "Memory used : 30.2709 GB Time per batch : 0.338s\n",
      "\n",
      "Model Hallucinator_s1 Epoch  0 : Iteration : 85/54120 loss value : 123.5312 \n",
      "Memory used : 30.2709 GB Time per batch : 0.348s\n",
      "\n",
      "Model Hallucinator_s1 Epoch  0 : Iteration : 90/54120 loss value : 133.7299 \n",
      "Memory used : 30.2712 GB Time per batch : 0.328s\n",
      "\n",
      "Model Hallucinator_s1 Epoch  0 : Iteration : 95/54120 loss value : 130.3209 \n",
      "Memory used : 30.2712 GB Time per batch : 0.353s\n",
      "\n",
      "Model Hallucinator_s1 Epoch  0 : Iteration : 100/54120 loss value : 130.7832 \n",
      "Memory used : 30.2712 GB Time per batch : 0.337s\n",
      "\n",
      "Model Hallucinator_s1 Epoch  0 : Iteration : 105/54120 loss value : 141.1281 \n",
      "Memory used : 30.2712 GB Time per batch : 0.359s\n",
      "\n",
      "Model Hallucinator_s1 Epoch  0 : Iteration : 110/54120 loss value : 129.9223 \n",
      "Memory used : 30.2712 GB Time per batch : 0.330s\n",
      "\n",
      "Model Hallucinator_s1 Epoch  0 : Iteration : 115/54120 loss value : 128.6440 \n",
      "Memory used : 30.2712 GB Time per batch : 0.343s\n",
      "\n",
      "Model Hallucinator_s1 Epoch  0 : Iteration : 120/54120 loss value : 122.2012 \n",
      "Memory used : 30.2713 GB Time per batch : 0.321s\n",
      "\n",
      "Model Hallucinator_s1 Epoch  0 : Iteration : 125/54120 loss value : 124.7350 \n",
      "Memory used : 30.2714 GB Time per batch : 0.336s\n",
      "\n",
      "Model Hallucinator_s1 Epoch  0 : Iteration : 130/54120 loss value : 128.3085 \n",
      "Memory used : 30.2714 GB Time per batch : 0.334s\n",
      "\n",
      "Model Hallucinator_s1 Epoch  0 : Iteration : 135/54120 loss value : 131.2507 \n",
      "Memory used : 30.2714 GB Time per batch : 0.339s\n",
      "\n",
      "Model Hallucinator_s1 Epoch  0 : Iteration : 140/54120 loss value : 126.5932 \n",
      "Memory used : 30.2726 GB Time per batch : 0.326s\n",
      "\n",
      "Model Hallucinator_s1 Epoch  0 : Iteration : 145/54120 loss value : 135.2429 \n",
      "Memory used : 30.2726 GB Time per batch : 0.348s\n",
      "\n",
      "Model Hallucinator_s1 Epoch  0 : Iteration : 150/54120 loss value : 126.4431 \n",
      "Memory used : 30.2726 GB Time per batch : 0.343s\n",
      "\n",
      "Model Hallucinator_s1 Epoch  0 : Iteration : 155/54120 loss value : 131.4018 \n",
      "Memory used : 30.2726 GB Time per batch : 0.330s\n",
      "\n",
      "Model Hallucinator_s1 Epoch  0 : Iteration : 160/54120 loss value : 121.4357 \n",
      "Memory used : 30.2726 GB Time per batch : 0.338s\n",
      "\n",
      "Model Hallucinator_s1 Epoch  0 : Iteration : 165/54120 loss value : 138.8520 \n",
      "Memory used : 30.2727 GB Time per batch : 0.335s\n",
      "\n",
      "Model Hallucinator_s1 Epoch  0 : Iteration : 170/54120 loss value : 137.3719 \n",
      "Memory used : 30.2727 GB Time per batch : 0.345s\n",
      "\n",
      "Model Hallucinator_s1 Epoch  0 : Iteration : 175/54120 loss value : 109.4513 \n",
      "Memory used : 30.2727 GB Time per batch : 0.325s\n",
      "\n",
      "Model Hallucinator_s1 Epoch  0 : Iteration : 180/54120 loss value : 127.3417 \n",
      "Memory used : 30.2727 GB Time per batch : 0.341s\n",
      "\n",
      "Model Hallucinator_s1 Epoch  0 : Iteration : 185/54120 loss value : 128.1470 \n",
      "Memory used : 30.2727 GB Time per batch : 0.329s\n",
      "\n",
      "Model Hallucinator_s1 Epoch  0 : Iteration : 190/54120 loss value : 131.6566 \n",
      "Memory used : 30.2727 GB Time per batch : 0.360s\n",
      "\n",
      "Model Hallucinator_s1 Epoch  0 : Iteration : 195/54120 loss value : 119.0607 \n",
      "Memory used : 30.2727 GB Time per batch : 0.345s\n",
      "\n",
      "Model Hallucinator_s1 Epoch  0 : Iteration : 200/54120 loss value : 127.7727 \n",
      "Memory used : 30.2727 GB Time per batch : 0.341s\n",
      "\n",
      "Model Hallucinator_s1 Epoch  0 : Iteration : 205/54120 loss value : 120.6904 \n",
      "Memory used : 30.2727 GB Time per batch : 0.337s\n",
      "\n",
      "Model Hallucinator_s1 Epoch  0 : Iteration : 210/54120 loss value : 127.1201 \n",
      "Memory used : 30.2727 GB Time per batch : 0.348s\n",
      "\n",
      "Model Hallucinator_s1 Epoch  0 : Iteration : 215/54120 loss value : 126.5637 \n",
      "Memory used : 30.2728 GB Time per batch : 0.347s\n",
      "\n",
      "Model Hallucinator_s1 Epoch  0 : Iteration : 220/54120 loss value : 131.1613 \n",
      "Memory used : 30.2729 GB Time per batch : 0.329s\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-591dc888e11a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHallucinator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'config_test.ini'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mH\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mH\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtestAll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-c479966f199e>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    239\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataObj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaxEpoch\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m             \u001b[0miters\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m             \u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataObj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnextTrainBatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m             \u001b[0mt1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt_summaries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTrainables\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss_summary\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdepth\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mphase\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrgb\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mgt\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Hallucination/src/Dataset.py\u001b[0m in \u001b[0;36mnextTrainBatch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0mgt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_rgb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0minp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadImages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m         \u001b[0mgt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadImages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatchSize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Hallucination/src/Dataset.py\u001b[0m in \u001b[0;36mloadImages\u001b[0;34m(self, imgs)\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mimg_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimgs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m             \u001b[0mimg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m             \u001b[0mimg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimgWidth\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimgHeight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__=='__main__': \n",
    "    H = Hallucinator('config_test.ini',1,0)\n",
    "    H.train()\n",
    "    H.testAll()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
